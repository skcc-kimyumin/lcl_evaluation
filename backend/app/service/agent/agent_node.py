# ìƒíƒœ í´ë˜ìŠ¤ ì •ì˜
from functools import partial
from operator import itemgetter
from typing import Dict

import openai
from core.config import get_setting
from database.repository.chat_history import ChatHistory
from fastapi import HTTPException
from langchain.chains.llm import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate,
)
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from log.logging import get_logging
from pymilvus import MilvusClient
from service.agent.prompts import basic_system_prompt_with_vector_search
from service.model.agent import ChatRequest, ChatState
from service.vectordb.milvus import search_vectors_info
from sqlalchemy.orm import Session

settings = get_setting()
logger = get_logging()

OPENAI_API_KEY = settings.OPENAI_API_KEY
USER_ID = settings.USER_ID


def workflow_builder1(request: ChatRequest, db: Session):
    graph = StateGraph(ChatState)

    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("input", input_node)
    graph.add_node("llm", llm_node)
    graph.add_node("output", partial(output_node, user_id=USER_ID, db=db))

    # ë…¸ë“œ ê°„ ì—°ê²° ì„¤ì • (Edges)
    graph.set_entry_point("input")
    graph.add_edge("input", "llm")
    graph.add_edge("llm", "output")

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app_graph = graph.compile()

    # LangGraph ì‹¤í–‰
    result = app_graph.invoke(request)

    return result


def workflow_builder2(request: ChatRequest, collection_name: str, db: Session, milvus: MilvusClient):
    graph = StateGraph(ChatState)

    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("input", input_node)
    graph.add_node("search_vector", partial(vector_node, collection_name=collection_name, milvus=milvus))
    graph.add_node("llm", partial(llm_node_lcel, user_id=USER_ID, db=db))
    graph.add_node("output", partial(output_node, user_id=USER_ID, db=db))

    # ë…¸ë“œ ê°„ ì—°ê²° ì„¤ì • (Edges)
    graph.set_entry_point("input")
    graph.add_edge("input", "search_vector")

    graph.add_edge("search_vector", "llm")
    graph.add_edge("llm", "output")

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app_graph = graph.compile()

    # LangGraph ì‹¤í–‰
    result = app_graph.invoke(request)

    return result


def workflow_builder3(request: ChatRequest, collection_name: str, db: Session, milvus: MilvusClient, memory: ConversationBufferMemory):
    graph = StateGraph(ChatState)
    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("input", input_node)
    graph.add_node("search_vector", partial(vector_node, collection_name=collection_name, milvus=milvus))
    graph.add_node("llm", partial(llm_node_lcel_with_memory, user_id=USER_ID, db=db, memory=memory))
    graph.add_node("output", partial(output_node, user_id=USER_ID, db=db))

    # ë…¸ë“œ ê°„ ì—°ê²° ì„¤ì • (Edges)
    graph.set_entry_point("input")
    graph.add_edge("input", "search_vector")
    graph.add_edge("search_vector", "llm")
    graph.add_edge("llm", "output")

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app_graph = graph.compile()

    # LangGraph ì‹¤í–‰
    result = app_graph.invoke(request)

    return result


def input_node(state: "ChatState") -> "ChatState":
    print(f"Input Node - Message received: {state.message}")
    return state


def vector_node(state: "ChatState", collection_name: str, milvus: MilvusClient):
    try:
        search_result = search_vectors_info(milvus=milvus, query=state.message, collection_name=collection_name)
        state.vector_result = search_result[0][0]["entity"]["text"]  # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼ stateì— ì €ì¥
        return state
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def llm_node(state: "ChatState"):
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": state.message}],
        )
        state.response = response.choices[0].message.content
        return state
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def llm_node_lcel(state: "ChatState", user_id: str, db: Session):
    try:
        # OpenAI LLM ì‚¬ìš©
        llm = ChatOpenAI(model="gpt-3.5-turbo", api_key=OPENAI_API_KEY)

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", basic_system_prompt_with_vector_search),
                MessagesPlaceholder(variable_name="user_message"),
                ("assistant", f"Vector search result: {state.vector_result}"), # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ í™œìš©
                # ("user", state.message),
            ]
        )

        # chain ìƒì„±
        chain = prompt | llm

        # í˜¸ì¶œ
        # result = chain.invoke() # placeholder ì•ˆì“¸ë•Œ
        result = chain.invoke({"user_message": [{"role": "user", "content": state.message}]})

        # ê²°ê³¼ë¥¼ stateì— ë°˜ì˜
        state.response = result.content

        return state

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def llm_node_lcel_with_memory(state: "ChatState", user_id: str, db: Session, memory: ConversationBufferMemory):
    try:
        # OpenAI LLM ì‚¬ìš©
        llm = ChatOpenAI(model="gpt-3.5-turbo", api_key=OPENAI_API_KEY)

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "ì¹œì ˆíˆ ë‹µë³€í•´ì¤˜"),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
                ("assistant", f"Vector search result: {state.vector_result}"), # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ í™œìš©
            ]
        )

        # ì´ì „ memory ìœ ì§€
        # if memory is None:
        #     memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")

        # ë©”ëª¨ë¦¬ì— ì €ì¥ëœ Dictë¥¼ "chat_history"ë¼ëŠ” keyë¡œ ê°’ ì „ë‹¬, ìœ„ì— ì„ ì–¸ëœ promptì—ì„œ variable_name="chat_hisotry"ì—ì„œ ê°’ ë°›ì•„ì„œ promptì— ëŒ€í™”ì´ë ¥ì„ ë„£ì–´ì¤Œ 
        runnable = RunnablePassthrough.assign(
            chat_history=RunnableLambda(memory.load_memory_variables)
            | itemgetter("chat_history")  # memory_key ì™€ ë™ì¼í•˜ê²Œ ì…ë ¥í•©ë‹ˆë‹¤.
        )

        # chain ìƒì„±
        chain = runnable | prompt | llm

        # í˜¸ì¶œ
        result = chain.invoke({"input": state.message})

        # ê²°ê³¼ë¥¼ stateì— ë°˜ì˜
        state.response = result.content

        memory.save_context(
            {"human": state.message}, {"ai": state.response}
        )

        return state

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def output_node(state: "ChatState", user_id: str, db: Session) -> Dict:
    print(f"Output Node - Response: {state.response}")

    # ë°ì´í„° ì´ë ¥ ì €ì¥
    chat_history = ChatHistory(
        user_id=user_id,
        message=state.message,
        response=state.response,
    )

    db.add(chat_history)
    db.commit()

    return {"response": state.response}


def best_pratice(request, collection_name: str, db, milvus) -> Dict:
    # LLM ì„¤ì •
    llm = ChatOpenAI(model="gpt-4")

    def get_next_step_from_plan(state):
        return state.get("next_step", "generate")    

    # 0. Planner Agent (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
    planner_prompt = PromptTemplate.from_template(
        """ì‚¬ìš©ìì˜ ì§ˆë¬¸: {query}

ë‹¤ìŒ ì‘ì—…ì„ ê³„íší•˜ê³ , ë‹¤ìŒ ë‹¨ê³„ í•˜ë‚˜ë¥¼ ê²°ì •í•´ì¤˜.
ê°€ëŠ¥í•œ ë‹¤ìŒ ë‹¨ê³„ëŠ” retrieve, rewrite, retrieve, generate, reflect, end ì¤‘ í•˜ë‚˜ì•¼.
- ì¸ì‚¬ë‚˜ ì¼ìƒ ëŒ€í™” ê°™ì€ ê°„ë‹¨í•œ ì§ˆë¬¸ì´ë©´ generateë¥¼ ì„ íƒí•´.
- ì¿¼ë¦¬ ì •ì œê°€ í•„ìš”í•˜ë©´ rewriteë¥¼ ì„ íƒí•´.
- ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ retrieveë¥¼ ì„ íƒí•´.

í˜•ì‹:
ê³„íš: <ê³„íš ë‚´ìš©>
ë‹¤ìŒ ë‹¨ê³„: <retrieve|rewrite|generate>
""")
    planner_chain = LLMChain(llm=llm, prompt=planner_prompt)

    def planner_node(state):
        logger.info("=================Planner node ì‹œì‘ =================")
        query_for_planner = state.get("rewritten_query") or state["query"]
        output = planner_chain.run(query=query_for_planner)
        lines = output.strip().splitlines()
        plan = "\n".join([line for line in lines if not line.lower().startswith("ë‹¤ìŒ ë‹¨ê³„:")])
        next_step_line = next((line for line in lines if line.lower().startswith("ë‹¤ìŒ ë‹¨ê³„:")), None)
        next_step = next_step_line.split(":", 1)[1].strip().lower() if next_step_line else "generate"
        logger.info(f"Plan:\n{plan}")
        logger.info(f"â¡ï¸ Next Step: {next_step}")
        return {**state, "plan": plan, "next_step": next_step}

    # 1. Rewriter Agent, ì¼ìƒì§ˆë¬¸ì¼ ê²½ìš° í•´ë‹¹ ë‹¨ê³„ ì—†ì´ ë°”ë¡œ generator
    rewriter_prompt = PromptTemplate.from_template(
        "ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸: {query}\nì´ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹¤ì‹œ í‘œí˜„í•´ì¤˜."
    )
    rewriter_chain = LLMChain(llm=llm, prompt=rewriter_prompt)

    def rewriter_node(state):
        logger.info("=================Rewriter node ì‹œì‘ =================")
        rewritten = rewriter_chain.run(query=state["query"])
        logger.info(f"ğŸ” Rewritten Query: {rewritten}")
        return {**state, "rewritten_query": rewritten, "next_step": "retrieve"}    

    # 2. Retriever Agent
    def retriever_node(state):
        logger.info("=================Retriever node ì‹œì‘ =================")
        try:
            search_result = search_vectors_info(milvus=milvus, query=state["query"], collection_name=collection_name)
            parsed = search_result[0][0]["entity"]["text"]
            # logger.info(f"Retrieved Document:\n{parsed}")
            return {**state, "documents": parsed, "next_step": "generate"}
        except Exception as e:
            logger.exception("Retriever Error")
            raise HTTPException(status_code=500, detail=str(e))

    # 3. Generator Agent
    generator_prompt = PromptTemplate.from_template(
        "ì‚¬ìš©ì ì§ˆë¬¸: {query}\nê´€ë ¨ ë¬¸ì„œ: {documents}\nì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•´ì¤˜."
    )
    generator_chain = LLMChain(llm=llm, prompt=generator_prompt)

    def generator_node(state):
        logger.info("=================Generator node ì‹œì‘ =================")
        query_for_generator = state.get("rewritten_query") or state["query"]
        response = generator_chain.run(
            query=query_for_generator,
            documents="\n".join(state.get("documents", []))
        )
        logger.info(f"ğŸ“ Response:\n{response}")
        return {**state, "response": response, "next_step": "reflect"}

    # 4. Reflector Agent
    reflector_prompt = PromptTemplate.from_template(
        """ì§ˆë¬¸: {query}\n ì‘ë‹µ: {response}\n
        ì§ˆë¬¸ì´ ì¸ì‚¬ë‚˜ ì¼ìƒ ëŒ€í™” ê°™ì€ ê°„ë‹¨í•œ ì§ˆë¬¸ì´ë©´ ë¬´ì¡°ê±´ OKë¼ê³  ë‹µë³€í•´ì¤˜.
        ê·¸ ì™¸ì˜ ì§ˆë¬¸ì´ë¼ë©´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í‰ê°€í•˜ëŠ”ê²Œ ë„ˆì˜ ì—­í• ì´ì•¼.
        ë¶€ì¡±í•˜ê±°ë‚˜ ê°œì„ í•  ì ì´ ìˆë‹¤ë©´ ì„¤ëª…í•˜ê³ , ê´œì°®ì€ ë‹µë³€ì´ë¼ë©´ OKë¼ê³  ë‹µë³€í•´ì¤˜."""
    )
    reflector_chain = LLMChain(llm=llm, prompt=reflector_prompt)

    def reflector_node(state):
        logger.info("=================Reflector node ì‹œì‘ =================")
        query_for_reflector = state.get("rewritten_query") or state["query"]
        feedback = reflector_chain.run(response=state["response"], query=query_for_reflector)
        next_step = "end" if "OK" in feedback else "generate"
        logger.info(f"Feedback:\n{feedback}")
        logger.info(f"â¡ï¸ Feedback judged next_step = {next_step}")
        return {**state, "feedback": feedback, "next_step": next_step}

    # ê·¸ë˜í”„ êµ¬ì„±
    graph_builder = StateGraph(dict)
    graph_builder.add_node("plan", RunnableLambda(planner_node))
    graph_builder.add_node("rewrite", RunnableLambda(rewriter_node))
    graph_builder.add_node("retrieve", RunnableLambda(retriever_node))
    graph_builder.add_node("generate", RunnableLambda(generator_node))
    graph_builder.add_node("reflect", RunnableLambda(reflector_node))

    graph_builder.set_entry_point("plan")
    graph_builder.add_conditional_edges("plan", get_next_step_from_plan, {
        "rewrite": "rewrite",
        "retrieve": "retrieve",
        "generate": "generate",
        "reflect": "reflect",
        "end": END,
    })
    graph_builder.add_edge("rewrite", "plan")
    graph_builder.add_edge("retrieve", "generate")
    graph_builder.add_edge("generate", "reflect")
    graph_builder.add_conditional_edges("reflect", get_next_step_from_plan, {
        "generate": "generate",
        "end": END,
    })

    # ì‹¤í–‰
    try:
        app = graph_builder.compile()
        initial_state = {"query": request.message}
        result = app.invoke(initial_state)
    except Exception as e:
        logger.exception("Invoke error")
        raise HTTPException(status_code=500, detail=str(e))

    logger.info(f"Final Response: {result['response']}")
    logger.info(f"Final Feedback: {result.get('feedback', 'N/A')}")
    return result
